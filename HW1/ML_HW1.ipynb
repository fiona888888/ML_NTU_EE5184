{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mz0_QVkxCrX3"
      },
      "source": [
        "# **Homework 1: COVID-19 Cases Prediction (Regression)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeZnPAiwDRWG"
      },
      "source": [
        "Author: Heng-Jui Chang\n",
        "\n",
        "Slides: https://github.com/ga642381/ML2021-Spring/blob/main/HW01/HW01.pdf  \n",
        "Videos (Mandarin): https://cool.ntu.edu.tw/courses/4793/modules/items/172854  \n",
        "https://cool.ntu.edu.tw/courses/4793/modules/items/172853  \n",
        "Video (English): https://cool.ntu.edu.tw/courses/4793/modules/items/176529\n",
        "\n",
        "\n",
        "Objectives:\n",
        "* Solve a regression problem with deep neural networks (DNN).\n",
        "* Understand basic DNN training tips.\n",
        "* Get familiar with PyTorch.\n",
        "\n",
        "If any questions, please contact the TAs via TA hours, NTU COOL, or email.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jx3x1nDkG-Uy"
      },
      "source": [
        "# **Download Data**\n",
        "\n",
        "\n",
        "If the Google drive links are dead, you can download data from [kaggle](https://www.kaggle.com/c/ml2021spring-hw1/data), and upload data manually to the workspace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMj55YDKG6ch",
        "outputId": "fc40ecc9-4756-48b1-d5c6-c169a8b453b2"
      },
      "outputs": [],
      "source": [
        "tr_path = './DataSet/covid.train.csv'  # path to training data\n",
        "tt_path = './DataSet/covid.test.csv'   # path to testing data\n",
        "\n",
        "#!gdown --id '19CCyCgJrUxtvgZF53vnctJiOJ23T5mqF' --output covid.train.csv\n",
        "#!gdown --id '1CE240jLm2npU-tdz81-oVKEF3T2yfT1O' --output covid.test.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wS_4-77xHk44"
      },
      "source": [
        "# **Import Some Packages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "k-onQd4JNA5H"
      },
      "outputs": [],
      "source": [
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# For data preprocess\n",
        "import numpy as np\n",
        "import csv\n",
        "import os\n",
        "\n",
        "# For plotting\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "# Sklearn\n",
        "import sklearn\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import f_regression\n",
        "from sklearn import preprocessing\n",
        "\n",
        "# Pandas\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Set seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of CUDA devices: 1\n"
          ]
        }
      ],
      "source": [
        "myseed = 42069  # set a random seed for reproducibility\n",
        "torch.backends.cudnn.deterministic = True  # with deterministic algorithms, You get the same outputs every time you train your model, Some operations may run slower, since PyTorch avoids faster but non-deterministic versions\n",
        "torch.backends.cudnn.benchmark = False  #‚û°Ô∏è PyTorch will skip benchmarking and just use a safe, deterministic algorithm (usually IMPLICIT_GEMM or WINOGRAD_NONFUSED).‚úÖ Reproducible results‚ùå Possibly slower than optimal\n",
        "np.random.seed(myseed)  \n",
        "torch.manual_seed(myseed)  # Sets the seed for CPU-based random operations in PyTorch.\n",
        "if torch.cuda.is_available():  # If CUDA is available, set the seed for GPU-based random operations as well.\n",
        "    torch.cuda.manual_seed_all(myseed)\n",
        "\n",
        "device_count = torch.cuda.device_count()\n",
        "print(f\"Number of CUDA devices: {device_count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtE3b6JEH7rw"
      },
      "source": [
        "# **Some Utilities**\n",
        "\n",
        "You do not need to modify this part."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "FWMT3uf1NGQp"
      },
      "outputs": [],
      "source": [
        "def get_device():\n",
        "    ''' Get device (if GPU is available, use GPU) '''\n",
        "    return 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# learning curve and validation plot\n",
        "def plot_learning_curve(loss_record, title=''):\n",
        "    ''' Plot learning curve of your DNN (train & dev loss) '''\n",
        "    total_steps = len(loss_record['train'])  # total training epochs\n",
        "    x_1 = range(total_steps)  # ex: list(range(1000))\n",
        "    x_2 = x_1[::len(loss_record['train']) // len(loss_record['dev'])]\n",
        "    figure(figsize=(6, 4))\n",
        "    plt.plot(x_1, loss_record['train'], c='tab:red', label='train')\n",
        "    plt.plot(x_2, loss_record['dev'], c='tab:cyan', label='dev')\n",
        "    plt.ylim(0.0, 5.)\n",
        "    plt.xlabel('Training steps')\n",
        "    plt.ylabel('MSE loss')\n",
        "    plt.title('Learning curve of {}'.format(title))\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_pred(dv_set, model, device, lim=35., preds=None, targets=None):\n",
        "    ''' Plot prediction of your DNN '''\n",
        "    if preds is None or targets is None:  # get predictions and targets from dataloader\n",
        "        model.eval()\n",
        "        preds, targets = [], []\n",
        "        for x, y in dv_set:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            with torch.no_grad():\n",
        "                pred = model(x)\n",
        "                preds.append(pred.detach().cpu())  # detach from GPU and convert to CPU  # collect it batch by batch\n",
        "                targets.append(y.detach().cpu()) \n",
        "        preds = torch.cat(preds, dim=0).numpy()   # list of tensors to a single tensor\n",
        "        targets = torch.cat(targets, dim=0).numpy()\n",
        "\n",
        "    figure(figsize=(5, 5))\n",
        "    plt.scatter(targets, preds, c='r', alpha=0.5)   # alpha=0.5 makes the points semi-transparent. see overlapping points better\n",
        "    plt.plot([-0.2, lim], [-0.2, lim], c='b')   # plot a line y=x, which is the ideal prediction line\n",
        "    plt.xlim(-0.2, lim)\n",
        "    plt.ylim(-0.2, lim)\n",
        "    plt.xlabel('ground truth value')\n",
        "    plt.ylabel('predicted value')\n",
        "    plt.title('Ground Truth v.s. Prediction')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Notes  \n",
        "##### **1. preds.append(pred.detach().cpu())**  \n",
        "we originally move the data to GPU \n",
        "This tensor:\n",
        "- Is usually on the GPU (if x was on GPU)\n",
        "- Is attached to the computation graph (so gradients can be calculated)  \n",
        "we need to:\n",
        "- Removes the tensor from the computation graph.\n",
        "- This tells PyTorch: \"I'm not going to backpropagate through this tensor.\"\n",
        "- It's essential to avoid memory leaks or unnecessary gradient tracking during inference.  \n",
        "* Moves the tensor from **GPU to CPU**, so you can safely:\n",
        "\n",
        "  * Collect it in a list\n",
        "  * Convert it to NumPy\n",
        "  * Plot or save it  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### **2. preds = torch.cat(preds, dim=0).numpy()**  \n",
        "Now `preds` is a **list of tensors** ‚Äî one tensor per batch. Example:\n",
        "\n",
        "```python\n",
        "preds = [Tensor(batch1), Tensor(batch2), Tensor(batch3)]\n",
        "```\n",
        "\n",
        "üß† Why `torch.cat(preds, dim=0)`?\n",
        "\n",
        "* Concatenates all batch tensors into a **single tensor**.\n",
        "* `dim=0` (row) joins them along the batch axis (like stacking rows).\n",
        "\n",
        "Example:\n",
        "\n",
        "```python\n",
        "# Suppose 3 batches of shape (2, 1)\n",
        "preds = [tensor([[0.1], [0.2]]),\n",
        "         tensor([[0.3], [0.4]]),\n",
        "         tensor([[0.5], [0.6]])]\n",
        "\n",
        "# After torch.cat(preds, dim=0)\n",
        "tensor([[0.1],\n",
        "        [0.2],\n",
        "        [0.3],\n",
        "        [0.4],\n",
        "        [0.5],\n",
        "        [0.6]])\n",
        "```\n",
        "\n",
        "üß† Why `.numpy()`?\n",
        "\n",
        "* Converts the final PyTorch tensor to a **NumPy array**.\n",
        "* NumPy arrays are easier to:\n",
        "\n",
        "  * Use in `matplotlib` plots\n",
        "  * Save with `np.save()`\n",
        "  * Compute with `sklearn` or other libraries\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Feature Selection**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                 Specs          Score\n",
            "75   tested_positive.1  148069.658278\n",
            "57     tested_positive   69603.872591\n",
            "42        hh_cmnty_cli    9235.492094\n",
            "60      hh_cmnty_cli.1    9209.019558\n",
            "78      hh_cmnty_cli.2    9097.375172\n",
            "43      nohh_cmnty_cli    8395.421300\n",
            "61    nohh_cmnty_cli.1    8343.255927\n",
            "79    nohh_cmnty_cli.2    8208.176435\n",
            "40                 cli    6388.906849\n",
            "58               cli.1    6374.548000\n",
            "76               cli.2    6250.008702\n",
            "41                 ili    5998.922880\n",
            "59               ili.1    5937.588576\n",
            "77               ili.2    5796.947672\n",
            "92  worried_finances.2     833.613191\n",
            "74  worried_finances.1     811.916460\n",
            "56    worried_finances     788.076931\n",
            "87    public_transit.2     686.736539\n",
            "69    public_transit.1     681.562902\n",
            "51      public_transit     678.834789\n",
            "[75, 57, 42, 60, 78, 43, 61, 79, 40, 58, 76, 41, 59, 77, 92, 74, 56]\n"
          ]
        }
      ],
      "source": [
        "data = pd.read_csv(r'./DataSet/covid.train.csv')   \n",
        "x = data[data.columns[1:94]]\n",
        "y = data[data.columns[94]]\n",
        "\n",
        "# Normalisation\n",
        "x = (x - x.min()) / (x.max() - x.min())\n",
        "\n",
        "# Create an instance of SelectKBest without specifying 'k'\n",
        "bestfeatures = SelectKBest(score_func=f_regression)\n",
        "\n",
        "# Compute the scores for all features\n",
        "fit = bestfeatures.fit(x,y)\n",
        "\n",
        "# Convert the scores into a DataFrame\n",
        "dfscores = pd.DataFrame(fit.scores_)\n",
        "\n",
        "# Create a DataFrame of the column names\n",
        "dfcolumns = pd.DataFrame(x.columns)\n",
        "\n",
        "# Concatenate the two DataFrames for better visualization\n",
        "featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
        "\n",
        "# Name the DataFrame columns\n",
        "featureScores.columns = ['Specs','Score']  \n",
        "\n",
        "# Print the 20 rows with the highest scores\n",
        "print(featureScores.nlargest(20,'Score'))  \n",
        "\n",
        "# Print the index of the most important features\n",
        "top_rows = featureScores.nlargest(20, 'Score').index.tolist()[:17]\n",
        "print(top_rows)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **1. data = pd.read_csv(r'covid.train.csv')**\n",
        "By default, `pandas.read_csv()` assumes that **the first row is the header** (column names).\n",
        "If your CSV file **does not have headers**, you should tell pandas:\n",
        "\n",
        "```python\n",
        "df = pd.read_csv('your_file.csv', header=None)\n",
        "```\n",
        "use r'...' to avoid \\n new line...... ‚ÄúInterpret the string literally, don‚Äôt process backslashes.‚Äù\n",
        "\n",
        "#### üîÅ Compare with:\n",
        "\n",
        "| Path                 | Meaning                                   |\n",
        "| -------------------- | ----------------------------------------- |\n",
        "| `./file.csv`         | In the **current** folder                 |\n",
        "| `../file.csv`        | In the **parent** folder                  |\n",
        "| `/file.csv`          | In the **root** directory (on Unix/macOS) |\n",
        "| `C:\\folder\\file.csv` | Absolute path on Windows                  |\n",
        "\n",
        "\n",
        "#### Old way:\n",
        "\n",
        "```python\n",
        "data = pd.read_csv(\"file.csv\")\n",
        "x = data[data.columns[1:94]]\n",
        "y = data[data.columns[94]]\n",
        "```\n",
        "\n",
        "‚úÖ Keeps data in **DataFrame format**\n",
        "‚úÖ You can use **column names**, labels, and Pandas functionality\n",
        "‚úÖ Ideal for feature selection, visualization, etc.\n",
        "\n",
        "---\n",
        "\n",
        "#### New way:\n",
        "\n",
        "```python\n",
        "train_data = np.array(pd.read_csv(\"file.csv\"))\n",
        "```\n",
        "\n",
        "‚úÖ Converts directly to **NumPy array**\n",
        "‚ùå **No access to column names**\n",
        "‚úÖ May be slightly faster and more memory-efficient\n",
        "‚ùå You must use numeric indexing only (e.g. `train_data[:, 0:5]`)\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Min-max scaling**\n",
        "\n",
        "StandardScaler vs MinMax for Feature Selection?\n",
        "\n",
        "| Scaler                       | Keeps Order | Keeps Relative Spacing  | Affects Variance                    | Good for f\\_regression?                          |\n",
        "| ---------------------------- | ----------- | ----------------------- | ----------------------------------- | ------------------------------------------------ |\n",
        "| **MinMaxScaler**             | ‚úÖ Yes       | ‚úÖ Yes                   | ‚úÖ Compresses all features to \\[0,1] | ‚úÖ Safe, common                                   |\n",
        "| **StandardScaler (Z-score)** | ‚úÖ Yes       | ‚ùå No (centers around 0) | ‚úÖ Gives mean=0, std=1               | ‚úÖ Also fine, slightly more sensitive to outliers |\n",
        "| **No Scaling**               | ‚ùå No        | ‚ùå No                    | ‚ùå \n",
        "\n",
        "\n",
        "### **3. How to know which method to choose?**\n",
        "\n",
        "‚úÖ When to use `SelectKBest(f_regression)`:\n",
        "\n",
        "| Use It When...                                    | Because                                           |\n",
        "| ------------------------------------------------- | ------------------------------------------------- |\n",
        "| Your **target variable is continuous**            | `f_regression` is for regression tasks            |\n",
        "| You want a **fast, simple, interpretable** method | It‚Äôs efficient and easy to visualize              |\n",
        "| You want a **baseline** for feature importance    | Good first step before using more complex methods |\n",
        "| You're using a **linear model or neural network** | These are sensitive to irrelevant/noisy features  |\n",
        "---\n",
        "method to choose?\n",
        "| Goal                                       | Recommended Feature Selector  | Notes                                   |\n",
        "| ------------------------------------------ | ----------------------------- | --------------------------------------- |\n",
        "| **Regression task** (target is continuous) | `f_regression`                | Fast and interpretable                  |\n",
        "| **Classification task** (target is labels) | `chi2`, `mutual_info_classif` | Chi-squared needs non-negative features |\n",
        "| Want nonlinear dependency detection        | `mutual_info_regression`      | Captures more complex patterns          |\n",
        "| Want model-based selection                 | `Lasso`, `RandomForest`       | Slower but smarter (uses learning)      |\n",
        "| Want recursive feature elimination         | `RFE`, `RFECV`                | Wraps around an estimator               |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39U_XFX6KOoj"
      },
      "source": [
        "# **Preprocess**\n",
        "\n",
        "We have three kinds of datasets:\n",
        "* `train`: for training\n",
        "* `dev`: for validation\n",
        "* `test`: for testing (w/o target value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQ-MdwpLL7Dt"
      },
      "source": [
        "## **Dataset**\n",
        "\n",
        "The `COVID19Dataset` below does:\n",
        "* read `.csv` files\n",
        "* extract features\n",
        "* split `covid.train.csv` into train/dev sets\n",
        "* normalize features\n",
        "\n",
        "Finishing `TODO` below might make you pass medium baseline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "0zlpIp9ANJRU"
      },
      "outputs": [],
      "source": [
        "class COVID19Dataset(Dataset):   # input: path to csv file, mode (train/dev/test), target_only (if True, only load target features)\n",
        "    '''step1: Dataset for loading and preprocessing the COVID19 dataset converts it to a PyTorch tensor of type float32.\n",
        "   '''\n",
        "    def __init__(self,\n",
        "                 path,\n",
        "                 mode='train',\n",
        "                 target_only=False):\n",
        "        self.mode = mode\n",
        "\n",
        "        # Read data into numpy arrays\n",
        "        with open(path, 'r') as fp:  # open the csv file, 'r' means read mode, fp is file pointer'\n",
        "            data = list(csv.reader(fp))  # read the csv file into a list of lists (each row is a list)\n",
        "                 # data: [['state', 'day1', 'day2', ..., 'day3', 'tested_positive'], ...]\n",
        "                 # The first row is the header, so we skip it by slicing data[1:]\n",
        "            data = np.array(data[1:])[:, 1:].astype(float)\n",
        "        \n",
        "        if not target_only:\n",
        "            feats = list(range(93))  # All features (0-92)\n",
        "        else:\n",
        "            # TODO: Using 40 states & 2 tested_positive features (indices = 57 & 75)\n",
        "            #feats = list(range(40)) + [57, 75]  # Only state features and tested_positive features\n",
        "            # feats = [i for i in range(40)] + [57, 75]  # Only state features and tested_positive features\n",
        "            feats = [75, 57, 42, 60, 78, 43, 61, 79, 40, 58, 76, 41, 59, 77, 92, 74, 56] #‰∏äÈù¢ÊåëÈÄâÁöÑÊúÄ‰ºòÁâπÂæÅ\n",
        "        \n",
        "\n",
        "        if mode == 'test':\n",
        "            # Testing data\n",
        "            # data: 893 x 93 (40 states + day 1 (18) + day 2 (18) + day 3 (17))\n",
        "            data = data[:, feats]\n",
        "            self.data = torch.FloatTensor(data)  # mid output\n",
        "        else:\n",
        "            # Training data (train/dev sets)\n",
        "            # data: 2700 x 94 (40 states + day 1 (18) + day 2 (18) + day 3 (18))\n",
        "            target = data[:, -1]\n",
        "            data = data[:, feats]\n",
        "            \n",
        "            # Splitting training data into train & dev sets  (quick and reproducible way without using sklearn)\n",
        "            if mode == 'train':\n",
        "                indices = [i for i in range(len(data)) if i % 10 != 0]\n",
        "            elif mode == 'dev':\n",
        "                indices = [i for i in range(len(data)) if i % 10 == 0]\n",
        "            # If your data is ordered (e.g., time-series), this split may introduce bias, need to shuffle the data before splitting.\n",
        "\n",
        "\n",
        "            # Convert data into PyTorch tensors \n",
        "            self.data = torch.FloatTensor(data[indices])   # mid output # dimension change numpy array to PyTorch tensor\n",
        "            self.target = torch.FloatTensor(target[indices])  # mid output\n",
        "\n",
        "        # Normalize features (you may remove this part to see what will happen)\n",
        "        self.data[:, 40:] = \\\n",
        "            (self.data[:, 40:] - self.data[:, 40:].mean(dim=0, keepdim=True)) \\\n",
        "            / self.data[:, 40:].std(dim=0, keepdim=True)  # Z-score standardization (Standardizes each feature to mean 0, std 1)  # keepdim=True keeps the dimensions of the tensor, so the mean and std are broadcasted correctly across the features. make sure the dimensions match when performing operations like subtraction and division.\n",
        "\n",
        "        self.dim = self.data.shape[1]\n",
        "\n",
        "        print('Finished reading the {} set of COVID19 Dataset ({} samples found, each dim = {})'\n",
        "              .format(mode, len(self.data), self.dim))\n",
        "    '''step2: get a sample at an index'''\n",
        "    def __getitem__(self, index):\n",
        "        # Returns individual sample at a time\n",
        "        if self.mode in ['train', 'dev']:\n",
        "            # For training\n",
        "            return self.data[index], self.target[index]\n",
        "        else:\n",
        "            # For testing (no target)\n",
        "            return self.data[index]         # It allows PyTorch to access individual samples from your dataset when training or evaluating a model. The idx parameter is the index of the sample you want to access.\n",
        "        \n",
        "    '''step3: number of rows in the dataset'''\n",
        "    def __len__(self):\n",
        "        # Returns the size of the dataset\n",
        "        return len(self.data)      # # Returns the number of samples in the dataset. This is needed by PyTorch to know the dataset size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### init Getilem len 3 step\n",
        "### **1. torchvision.datasets.ImageFolder**\n",
        "\n",
        "You can use `torchvision.datasets.ImageFolder` when your image data is organized in directories where each subdirectory represents a class, and all images in that subdirectory belong to that class.\n",
        "\n",
        "In your ConcreteDataset, you manually convert a NumPy array to PyTorch tensors and implement the __getitem__ and __len__ methods because you are building a custom dataset from scratch (not using any built-in dataset class).  **need of the three steps**\n",
        "\n",
        "**When to use:**\n",
        "- When you have a folder structure like:\n",
        "  ```\n",
        "  root/\n",
        "    class1/\n",
        "      img1.jpg\n",
        "      img2.jpg\n",
        "      ...\n",
        "    class2/\n",
        "      img3.jpg\n",
        "      img4.jpg\n",
        "      ...\n",
        "    ...\n",
        "  ```\n",
        "- When you want to quickly load images and their labels for classification tasks.\n",
        "\n",
        "**What kind of data:**\n",
        "- Image classification datasets (multi-class or binary).\n",
        "- Each class has its own folder.\n",
        "- Supported image formats (e.g., .jpg, .png).\n",
        "\n",
        "**Example usage in your workspace:**  \n",
        "See HW4 copy.ipynb, where `datasets.ImageFolder` is used to load the `animals/train` and `animals/val` folders for multi-class classification (cats, dogs, pandas).\n",
        "\n",
        "### **2. self.data = torch.FloatTensor(data[indices])**\n",
        "Certainly! Here‚Äôs a concrete example for the line:\n",
        "\n",
        "```python\n",
        "self.data = torch.FloatTensor(data[indices])\n",
        "```\n",
        "\n",
        "Suppose your **original NumPy array** `data` looks like this (with 5 samples and 3 features):\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "data = np.array([\n",
        "    [1.0, 2.0, 3.0],   # sample 0\n",
        "    [4.0, 5.0, 6.0],   # sample 1\n",
        "    [7.0, 8.0, 9.0],   # sample 2\n",
        "    [10.0, 11.0, 12.0],# sample 3\n",
        "    [13.0, 14.0, 15.0] # sample 4\n",
        "])  # shape: (5, 3)\n",
        "```\n",
        "\n",
        "Suppose `indices = [0, 2, 4]` (selecting samples 0, 2, and 4):\n",
        "\n",
        "```python\n",
        "selected = data[indices]\n",
        "# selected =\n",
        "# array([[ 1.,  2.,  3.],\n",
        "#        [ 7.,  8.,  9.],\n",
        "#        [13., 14., 15.]])\n",
        "# shape: (3, 3)\n",
        "```\n",
        "\n",
        "Now, convert to a PyTorch tensor:\n",
        "\n",
        "```python\n",
        "import torch\n",
        "tensor_data = torch.FloatTensor(selected)\n",
        "# tensor_data =\n",
        "# tensor([[ 1.,  2.,  3.],\n",
        "#         [ 7.,  8.,  9.],\n",
        "#         [13., 14., 15.]])\n",
        "# shape: torch.Size([3, 3])\n",
        "```\n",
        "\n",
        "**Summary Table:**\n",
        "\n",
        "| Step                | Type         | Shape    | Example Values                |\n",
        "|---------------------|--------------|----------|------------------------------|\n",
        "| Original `data`     | np.ndarray   | (5, 3)   | see above                    |\n",
        "| `data[indices]`     | np.ndarray   | (3, 3)   | rows 0, 2, 4                 |\n",
        "| `torch.FloatTensor` | torch.Tensor | (3, 3)   | same values, type float32    |\n",
        "\n",
        "This is exactly what happens in your code for each split (train/dev/test), just with more samples and features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlhTlkE7MDo3"
      },
      "source": [
        "## **DataLoader**\n",
        "\n",
        "A `DataLoader` loads data from a given `Dataset` into batches.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "hlhLk5t6MBX3"
      },
      "outputs": [],
      "source": [
        "def prep_dataloader(path, mode, batch_size, n_jobs=0, target_only=False):\n",
        "    ''' Generates a dataset, then is put into a dataloader. '''\n",
        "    dataset = COVID19Dataset(path, mode=mode, target_only=target_only)  # Construct dataset\n",
        "    dataloader = DataLoader(\n",
        "        dataset, batch_size,\n",
        "        shuffle=(mode == 'train'), drop_last=False,  # mode is 'train', true, shuffle the data\n",
        "        num_workers=n_jobs, pin_memory=True)                            # Construct dataloader\n",
        "    return dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **1. drop last: \n",
        "##### If `drop_last=True`:\n",
        "* **Drops** the last smaller batch.\n",
        "* Using the same example, you get:\n",
        "\n",
        "  * One batch of 64\n",
        "  * **No batch of 36** ‚Äî it's dropped\n",
        "\n",
        "\n",
        "##### üìå Why use `drop_last=True`?\n",
        "\n",
        "##### ‚úÖ Common reasons:\n",
        "\n",
        "1. **For training**: It‚Äôs sometimes helpful to ensure all batches are the **same size** (e.g., for batch norm).\n",
        "2. **Avoid irregular shapes** in models that require fixed input sizes.\n",
        "3. In **multi-GPU training**, each GPU must receive the same batch size.\n",
        "\n",
        "---\n",
        "\n",
        "##### ‚ö†Ô∏è When NOT to use `drop_last=True`:\n",
        "\n",
        "* When you **want to use all data**, especially if your dataset is small.\n",
        "* During **evaluation/testing**, where accuracy matters more than batch consistency.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGuycwR0MeQB"
      },
      "source": [
        "# **Deep Neural Network**\n",
        "\n",
        "`NeuralNet` is an `nn.Module` designed for regression.\n",
        "The DNN consists of 2 fully-connected layers with ReLU activation.\n",
        "This module also included a function `cal_loss` for calculating loss.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "49-uXYovOAI0"
      },
      "outputs": [],
      "source": [
        "class NeuralNet(nn.Module):\n",
        "    ''' A simple fully-connected deep neural network '''\n",
        "    def __init__(self, input_dim):\n",
        "        super(NeuralNet, self).__init__()\n",
        "\n",
        "        # Define your neural network here\n",
        "        # TODO: How to modify this model to achieve better performance?\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 16),\n",
        "            nn.BatchNorm1d(16), #accelerates training by normalizing the inputs to each layer, which can help with convergence and stability.\n",
        "            nn.Dropout(p=0.2), # Dropout is a regularization technique that helps prevent overfitting by randomly setting a fraction of the input units to zero during training.\n",
        "            nn.ReLU(),\n",
        "            #nn.Linear(64, 32),\n",
        "            #nn.ReLU(),\n",
        "            nn.Linear(16, 1)\n",
        "        )\n",
        "\n",
        "        # Mean squared error loss\n",
        "        self.criterion = nn.MSELoss(reduction='mean')  # reduction='mean' means the loss is averaged over all elements in the batch. This is useful for regression tasks where you want to minimize the average error across all predictions. If you want to sum the losses instead, you can use reduction='sum'. If you want to return the loss for each element in the batch, you can use reduction='none'.\n",
        "\n",
        "    def forward(self, x):\n",
        "        ''' Given input of size (batch_size x input_dim), compute output of the network '''\n",
        "        return self.net(x).squeeze(1)  # (3. more explain) Squeeze removes the dimension of size 1 from the output tensor, which is useful when you want to convert a tensor of shape (batch_size, 1) to (batch_size,). This is often done in regression tasks where the output is a single value for each input sample.\n",
        "\n",
        "    def cal_loss(self, pred, target):\n",
        "        ''' Calculate loss # TODO: you may implement L1/L2 regularization here'''\n",
        "        regularization_loss = 0\n",
        "        for param in self.parameters():\n",
        "            regularization_loss += torch.sum(param ** 2) # L2 regularization\n",
        "        return self.criterion(pred, target) + 0.00075 * regularization_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Behind the scenes, this:\n",
        "\n",
        "```python\n",
        "model(x)\n",
        "```\n",
        "\n",
        "**automatically calls**:\n",
        "\n",
        "```python\n",
        "model.__call__(x)\n",
        "‚Üí self.forward(x)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **1. What is L2 Regularization?**\n",
        "\n",
        "**L2 regularization**, also known as **weight decay**, is a technique used to **reduce overfitting** in machine learning models by penalizing large weight values.\n",
        "\n",
        "üß† Formal Definition:\n",
        "\n",
        "For a model with weights $w$, the total loss becomes:\n",
        "\n",
        "$$\n",
        "\\text{Total Loss} = \\text{Original Loss} + \\lambda \\sum_{i} w_i^2\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $\\lambda$ is the **regularization strength** (a small constant like `0.00075`)\n",
        "* $\\sum w_i^2$ is the **sum of squares of all weights**\n",
        "* The regularization term discourages the model from making any weight too large\n",
        "\n",
        "‚úÖ In PyTorch Code:\n",
        "\n",
        "```python\n",
        "for param in model.parameters():\n",
        "    regularization_loss += torch.sum(param ** 2)\n",
        "\n",
        "loss = criterion(pred, target) + Œª * regularization_loss\n",
        "```\n",
        "\n",
        "---\n",
        "üìå Why use L2 regularization?\n",
        "\n",
        "| Reason                                   | Benefit                           |\n",
        "| ---------------------------------------- | --------------------------------- |\n",
        "| Prevents weights from becoming too large | Reduces model complexity          |\n",
        "| Helps avoid overfitting                  | Generalizes better to unseen data |\n",
        "| Keeps learning smooth                    | Encourages simpler solutions      |\n",
        "| Works well with gradient descent         | Differentiable and efficient      |\n",
        "\n",
        "---\n",
        "\n",
        "üîç L1 vs. L2 Regularization (Quick Comparison)\n",
        "\n",
        "| Feature     | **L1**                      | **L2**                        |   |                      |\n",
        "| ----------- | --------------------------- | ----------------------------- | - | -------------------- |\n",
        "| Formula     | ( \\lambda \\sum              | w\\_i                          | ) | $\\lambda \\sum w_i^2$ |\n",
        "| Effect      | Sparsity (many weights = 0) | Shrinks all weights slightly  |   |                      |\n",
        "| Use case    | Feature selection           | Smooth, stable regularization |   |                      |\n",
        "| Also called | Lasso                       | Ridge                         |   |                      |\n",
        "\n",
        "\n",
        "Great question ‚Äî let‚Äôs break it down clearly so you understand **why L2 regularization causes weight decay** and how it helps your model.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. üí° What Is Weight Decay?**\n",
        "\n",
        "**Weight decay** is just another name for **L2 regularization**.\n",
        "\n",
        "The idea is:\n",
        "\n",
        "> Penalize large weights in your model by adding a small cost to the loss function for big weight values.\n",
        "\n",
        "This helps prevent **overfitting** and encourages the model to keep weights small and general.\n",
        "\n",
        "---\n",
        "\n",
        " üìâ How Does L2 Regularization Work?\n",
        "\n",
        "In L2 regularization, the **loss function** becomes:\n",
        "\n",
        "$$\n",
        "\\text{Total Loss} = \\text{Original Loss} + \\lambda \\sum_{i} w_i^2\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* `Original Loss` = e.g. MSE or Cross Entropy\n",
        "* `w_i` = each trainable weight\n",
        "* `Œª` (lambda) = regularization strength (e.g. `0.00075`)\n",
        "\n",
        "---\n",
        "\n",
        "üîÅ How It Causes Weight Decay\n",
        "\n",
        "During backpropagation, the optimizer updates the weights like this:\n",
        "\n",
        "$$\n",
        "w \\leftarrow w - \\text{lr} \\cdot \\left( \\frac{\\partial \\text{Loss}}{\\partial w} + \\lambda \\cdot w \\right)\n",
        "$$\n",
        "\n",
        "The extra term `+ Œª¬∑w` pulls the weight toward **zero**.\n",
        "\n",
        "- This is why it‚Äôs called **weight decay**:\n",
        "\n",
        "> Each weight shrinks a little every step ‚Üí \"decaying\" over time.\n",
        "\n",
        "---\n",
        "ü§î Why Add Weight Decay?\n",
        "üéØ Purpose:\n",
        "\n",
        "* **Reduce overfitting** on training data\n",
        "* **Improve generalization** to unseen data\n",
        "* Keep the model **simpler** by preventing very large weight values\n",
        "\n",
        "---\n",
        "\n",
        "‚úÖ Optimizer vs. Manual\n",
        "\n",
        "You have two ways to apply weight decay (L2):\n",
        "1. üîß Automatically with Optimizer\n",
        "\n",
        "```python\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.00075)\n",
        "```\n",
        "\n",
        "PyTorch will apply L2 during gradient updates.\n",
        "\n",
        "---\n",
        "\n",
        "**‚úçÔ∏è Manually in Loss Function**\n",
        "\n",
        "```python\n",
        "loss = criterion(pred, target) + lambda * sum(param**2 for param in model.parameters())\n",
        "```\n",
        "\n",
        "But don‚Äôt **do both** ‚Äî you‚Äôll apply L2 **twice**!\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### **3. self.net(x).squeeze(1)**\n",
        "The code in question is:\n",
        "```python\n",
        "self.net(x).squeeze(1)\n",
        "```\n",
        "This is from the `forward` method of your `NeuralNet` class.\n",
        "\n",
        "---\n",
        "\n",
        "#### What does it do?\n",
        "\n",
        "- `self.net(x)` passes the input tensor `x` through the neural network (`self.net`), producing an output tensor.\n",
        "- The output shape from the last layer is typically `(batch_size, 1)` (since you have `nn.Linear(..., 1)` as the last layer).\n",
        "- `.squeeze(1)` removes the dimension at index 1 **if it is size 1**.\n",
        "\n",
        "---\n",
        "\n",
        "#### What is the original size?\n",
        "\n",
        "- **Input:** `x` is usually of shape `(batch_size, input_dim)`.\n",
        "- **Output before squeeze:** After `self.net(x)`, the shape is `(batch_size, 1)`.\n",
        "\n",
        "---\n",
        "\n",
        "#### What is the result?\n",
        "\n",
        "- **After `.squeeze(1)`:** The shape becomes `(batch_size,)` (a 1D tensor with one value per sample in the batch).\n",
        "\n",
        "---\n",
        "\n",
        "**Summary Table:**\n",
        "\n",
        "| Step                | Shape                  |\n",
        "|---------------------|------------------------|\n",
        "| Input `x`           | `(batch_size, input_dim)` |\n",
        "| After `self.net(x)` | `(batch_size, 1)`         |\n",
        "| After `.squeeze(1)` | `(batch_size,)`           |\n",
        "\n",
        "This is commonly done for regression tasks, so the output matches the target shape."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvFWVjZ5Nvga"
      },
      "source": [
        "# **Train/Dev/Test**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAM8QecJOyqn"
      },
      "source": [
        "## **Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "lOqcmYzMO7jB"
      },
      "outputs": [],
      "source": [
        "def train(tr_set, dv_set, model, config, device):\n",
        "    ''' DNN training '''\n",
        "\n",
        "    n_epochs = config['n_epochs']  # Maximum number of epochs\n",
        "\n",
        "    # Setup optimizer\n",
        "    optimizer = getattr(torch.optim, config['optimizer'])(\n",
        "        model.parameters(), **config['optim_hparas'])  # **dict\tUnpack dictionary into keyword arguments\n",
        "\n",
        "    min_mse = 1000.  # or min_mse = float('inf')  # Initialize minimum MSE to a large value\n",
        "    loss_record = {'train': [], 'dev': []}      # for recording training loss\n",
        "    early_stop_cnt = 0\n",
        "    epoch = 0\n",
        "    while epoch < n_epochs:\n",
        "        model.train()                           # set model to training mode\n",
        "        for x, y in tr_set:                     # iterate through the dataloader (x is a batch of input features, y is a batch of target values)\n",
        "            optimizer.zero_grad()               # set gradient to zero (weights update one time in every batch, so we need to zero the gradients before each batch)\n",
        "            x, y = x.to(device), y.to(device)   # move data to device (cpu/cuda)\n",
        "            pred = model(x)                     # forward pass (compute output)\n",
        "            mse_loss = model.cal_loss(pred, y)  # compute loss\n",
        "            mse_loss.backward()                 # compute gradient (backpropagation)\n",
        "            optimizer.step()                    # update model with optimizer\n",
        "            loss_record['train'].append(mse_loss.detach().cpu().item())  # 1 loss value per batch\n",
        "\n",
        "        # After each epoch, test your model on the validation (development) set.\n",
        "        dev_mse = dev(dv_set, model, device) # the average loss of 1 epoch (unit is a sample)\n",
        "        if dev_mse < min_mse:\n",
        "            # Save model if your model improved\n",
        "            min_mse = dev_mse\n",
        "            print('Saving model (epoch = {:4d}, loss = {:.4f})'\n",
        "                .format(epoch + 1, min_mse))\n",
        "            torch.save(model.state_dict(), config['save_path'])  # Save model to specified path\n",
        "            early_stop_cnt = 0  # Reset early stop counter if model improved\n",
        "        else:\n",
        "            early_stop_cnt += 1\n",
        "\n",
        "        epoch += 1\n",
        "        loss_record['dev'].append(dev_mse)\n",
        "        if early_stop_cnt > config['early_stop']:\n",
        "            # Stop training if your model stops improving for \"config['early_stop']\" epochs.\n",
        "            break\n",
        "\n",
        "    print('Finished training after {} epochs'.format(epoch))\n",
        "    return min_mse, loss_record"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Important note**\n",
        "Your original code records the loss **with weight decay included** (i.e., MSE loss + L2 penalty) because:\n",
        "\n",
        "- **The total loss (MSE + L2 penalty) is what the model actually optimizes during training.**\n",
        "- Recording this \"full\" loss gives you a true picture of what the optimizer is minimizing.\n",
        "- If you only record the MSE loss, your learning curve might look better than the real optimization objective, and you might not notice if the regularization term is dominating or affecting convergence.\n",
        "\n",
        "**In summary:**  \n",
        "Recording the total loss (including weight decay) helps you monitor the real objective being minimized, making your training/validation curves more honest and comparable to the actual optimization process. This is especially important when tuning the regularization strength, as it lets you see its effect on the total loss.\n",
        "\n",
        "### **1. under the scene of each batch**\n",
        "#### üîÅ Full Update Cycle in Each Batch:\n",
        "\n",
        "In every **training batch**, this is what happens:\n",
        "\n",
        "```python\n",
        "# 1. Forward pass\n",
        "pred = model(x_batch)\n",
        "\n",
        "# 2. Loss computation\n",
        "loss = model.cal_loss(pred, y_batch)\n",
        "\n",
        "# 3. Backward pass: compute gradients\n",
        "loss.backward()\n",
        "\n",
        "# 4. Update weights using gradients\n",
        "optimizer.step()\n",
        "\n",
        "# 5. Clear accumulated gradients for the next batch\n",
        "optimizer.zero_grad()\n",
        "```\n",
        "#### 1. `.backward()`\n",
        "\n",
        "Fills:\n",
        "\n",
        "```python\n",
        "model.weight.grad  ‚Üí ‚àÇloss/‚àÇweight  (shape [1, 5])\n",
        "model.bias.grad    ‚Üí ‚àÇloss/‚àÇbias    (shape [1])\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "#### 2. `optimizer.step()`\n",
        "\n",
        "Does this for each parameter:\n",
        "\n",
        "```python\n",
        "# Example for SGD\n",
        "for param in model.parameters():\n",
        "    param.data -= lr * param.grad\n",
        "```\n",
        "\n",
        "In our earlier example:\n",
        "\n",
        "If:\n",
        "\n",
        "```python\n",
        "param.grad = tensor([0.5, -0.2, 0.1, 0.0, -0.3])\n",
        "param.data = tensor([0.0, 0.0, 0.0, 0.0, 0.0])\n",
        "```\n",
        "\n",
        "Then after `optimizer.step()` with `lr = 0.1`:\n",
        "\n",
        "```python\n",
        "param.data = tensor([-0.05, 0.02, -0.01, 0.0, 0.03])\n",
        "```\n",
        "\n",
        "Each parameter moves a little **in the direction that reduces the loss**.\n",
        "\n",
        "---\n",
        "\n",
        "#### üö® Important: No `.step()` Without `.backward()`\n",
        "\n",
        "If you skip `.backward()` or forget `loss.backward()`, then `.grad` will be `None`, and `.step()` does nothing.\n",
        "\n",
        "---\n",
        "\n",
        "#### üßπ Don‚Äôt Forget `.zero_grad()`\n",
        "\n",
        "Always clear gradients **before the next batch**, or they will accumulate:\n",
        "\n",
        "```python\n",
        "optimizer.zero_grad()\n",
        "```\n",
        "\n",
        "This resets all `param.grad` to 0.\n",
        "\n",
        "\n",
        "---\n",
        "#### **üîÅ Main Training Loop ‚Äî Math Under the Hood**\n",
        "\n",
        "We‚Äôll walk through:\n",
        "\n",
        "1. Forward pass\n",
        "2. Loss calculation (MSE + L2 regularization)\n",
        "3. Backward pass\n",
        "4. Parameter update\n",
        "5. Repeat for each batch\n",
        "\n",
        "---\n",
        "\n",
        "##### **1. Forward Pass**\n",
        "\n",
        "For a batch of input data $\\mathbf{X} \\in \\mathbb{R}^{N \\times d}$ (where $N$ is the batch size, and $d$ is the input dimension), the network computes:\n",
        "\n",
        "$$\n",
        "\\hat{\\mathbf{y}} = f(\\mathbf{X}; \\boldsymbol{\\theta})\n",
        "$$\n",
        "\n",
        "For your model with two layers:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\mathbf{h}_1 &= \\text{ReLU}(\\text{Dropout}(\\text{BatchNorm}(\\mathbf{X} \\mathbf{W}_1 + \\mathbf{b}_1))) \\\\\n",
        "\\hat{\\mathbf{y}} &= \\mathbf{h}_1 \\mathbf{W}_2 + \\mathbf{b}_2\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $\\mathbf{W}_1, \\mathbf{b}_1$: weights and bias of the first layer\n",
        "* $\\mathbf{W}_2, \\mathbf{b}_2$: weights and bias of the second layer\n",
        "* $\\boldsymbol{\\theta}$: all trainable parameters\n",
        "\n",
        "---\n",
        "\n",
        "##### **2. Loss Calculation (with L2 Regularization)**\n",
        "\n",
        " **a. Mean Squared Error (MSE) Loss**:\n",
        "\n",
        "$$\n",
        "\\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (\\hat{y}_i - y_i)^2\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $\\hat{y}_i$ is the prediction\n",
        "* $y_i$ is the ground truth\n",
        "\n",
        " **b. L2 Regularization Term**:\n",
        "\n",
        "$$\n",
        "\\text{L2 penalty} = \\lambda \\sum_{j} \\theta_j^2\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $\\lambda$: regularization strength\n",
        "* $\\theta_j$: each trainable weight (biases often excluded)\n",
        "\n",
        " **c. Total Loss**:\n",
        "\n",
        "$$\n",
        "\\text{Total Loss} = \\text{MSE} + \\lambda \\sum_j \\theta_j^2\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "##### **3. Backward Pass**\n",
        "\n",
        "PyTorch computes the gradient of the total loss with respect to each parameter:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\text{Total Loss}}{\\partial \\theta_j} = \\frac{\\partial \\text{MSE}}{\\partial \\theta_j} + 2 \\lambda \\theta_j\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "##### **4. Parameter Update**\n",
        "\n",
        "If using **SGD**, the update rule is:\n",
        "\n",
        "$$\n",
        "\\theta_j \\leftarrow \\theta_j - \\eta \\cdot \\frac{\\partial \\text{Total Loss}}{\\partial \\theta_j}\n",
        "$$\n",
        "\n",
        "If using **SGD with momentum**:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "v_j &\\leftarrow \\mu v_j + \\eta \\cdot \\frac{\\partial \\text{Total Loss}}{\\partial \\theta_j} \\\\\n",
        "\\theta_j &\\leftarrow \\theta_j - v_j\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $\\eta$: learning rate\n",
        "* $\\mu$: momentum factor\n",
        "* $v_j$: velocity term (momentum accumulator)\n",
        "\n",
        "---\n",
        "\n",
        "##### **5. Repeat for Each Batch and Epoch**\n",
        "\n",
        "This full loop repeats for every mini-batch in every epoch.\n",
        "\n",
        "---\n",
        "\n",
        "#### ‚úÖ Summary Table\n",
        "\n",
        "| Step           | Formula                                                              |\n",
        "| -------------- | -------------------------------------------------------------------- |\n",
        "| **Forward**    | $\\hat{y} = f(\\mathbf{X}; \\boldsymbol{\\theta})$                       |\n",
        "| **MSE Loss**   | $\\frac{1}{N} \\sum_{i=1}^N (\\hat{y}_i - y_i)^2$                       |\n",
        "| **L2 Penalty** | $\\lambda \\sum_j \\theta_j^2$                                          |\n",
        "| **Total Loss** | $\\text{MSE} + \\lambda \\sum_j \\theta_j^2$                             |\n",
        "| **Gradient**   | $\\frac{\\partial \\text{MSE}}{\\partial \\theta_j} + 2 \\lambda \\theta_j$ |\n",
        "| **Update**     | $\\theta_j \\leftarrow \\theta_j - \\eta \\cdot \\text{gradient}$          |\n",
        "\n",
        "---\n",
        "### **2. Why use `.detach().cpu().item()`**\n",
        "Because:\n",
        "\n",
        "* You **don‚Äôt want to track** the loss in autograd anymore\n",
        "* You want to **move it to CPU** (safe for logging)\n",
        "* You want it as a **float**, not a tensor\n",
        "\n",
        "### **3. Early stopping count**\n",
        "The **typical value for early stopping count (also called ‚Äúpatience‚Äù)** depends on your model, dataset, and how noisy your validation loss is ‚Äî but here's a general guideline:\n",
        "\n",
        "---\n",
        "\n",
        "#### ‚úÖ Common Ranges:\n",
        "\n",
        "| Scenario                               | Suggested `early_stop` Count |\n",
        "| -------------------------------------- | ---------------------------- |\n",
        "| Small model / low noise                | **3 to 5**                   |\n",
        "| Medium-sized / moderate noise          | **5 to 10**                  |\n",
        "| Large / deep models (e.g. CNNs, LSTMs) | **10 to 20+**                |\n",
        "\n",
        "---\n",
        "\n",
        "#### üìå Key Considerations:\n",
        "\n",
        "1. **Noise in validation loss:**\n",
        "\n",
        "   * If your `dev_mse` fluctuates a lot (due to randomness in batches, dropout, etc.), use **larger patience** to avoid stopping too early.\n",
        "\n",
        "2. **Epoch duration:**\n",
        "\n",
        "   * If one epoch is **slow to run**, you may prefer a smaller patience to save compute time.\n",
        "   * If one epoch is **fast**, a longer patience is acceptable.\n",
        "\n",
        "3. **Overfitting behavior:**\n",
        "\n",
        "   * If your model starts to overfit quickly, use **shorter patience** (e.g., 3‚Äì5).\n",
        "   * If your model takes a while to learn useful patterns, use **longer patience** (e.g., 10‚Äì15).\n",
        "\n",
        "4. **Learning rate schedule:**\n",
        "\n",
        "   * If you're using learning rate decay, early stopping patience should often be **longer than the decay step interval** so it has time to adapt.\n",
        "\n",
        "---\n",
        "\n",
        "#### üß† Good Default for Most DNNs:\n",
        "\n",
        "```python\n",
        "early_stop = 5\n",
        "```\n",
        "\n",
        "This allows 5 chances (epochs) to improve ‚Äî a balance between stopping too early and wasting time.\n",
        "\n",
        "---\n",
        "\n",
        "#### üß™ Optional Tip:\n",
        "\n",
        "Many frameworks (like `keras.callbacks.EarlyStopping`) even allow:\n",
        "\n",
        "```python\n",
        "restore_best_weights=True\n",
        "```\n",
        "\n",
        "which reloads the model weights from the **best** epoch when stopping occurs.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hSd4Bn3O2PL"
      },
      "source": [
        "## **Validation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "yrxrD3YsN3U2"
      },
      "outputs": [],
      "source": [
        "def dev(dv_set, model, device):\n",
        "    model.eval()                                # set model to evalutation mode\n",
        "    total_loss = 0\n",
        "    for x, y in dv_set:                         # iterate through the dataloader\n",
        "        x, y = x.to(device), y.to(device)       # move data to device (cpu/cuda)\n",
        "        with torch.no_grad():                   # disable gradient calculation\n",
        "            pred = model(x)                     # forward pass (compute output)\n",
        "            mse_loss = model.cal_loss(pred, y)  # compute loss\n",
        "        total_loss += mse_loss.detach().cpu().item() * len(x)  # accumulate loss for sum of 1 batch # sample is the unit\n",
        "    total_loss = total_loss / len(dv_set.dataset)              # compute averaged loss„ÄÄÔºàdivide by total samples in the dataset)\n",
        "\n",
        "    return total_loss  # averaged loss of 1 epoch(unit is a sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0pdrhQAO41L"
      },
      "source": [
        "## **Testing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "aSBMRFlYN5tB"
      },
      "outputs": [],
      "source": [
        "def test(tt_set, model, device):\n",
        "    model.eval()                                # set model to evalutation mode\n",
        "    preds = []\n",
        "    for x in tt_set:                            # iterate through the dataloader\n",
        "        x = x.to(device)                        # move data to device (cpu/cuda)\n",
        "        with torch.no_grad():                   # disable gradient calculation\n",
        "            pred = model(x)                     # forward pass (compute output) # tensor of predictions for one batch: tensor([12.5,  8.3, 15.7,  9.1])\n",
        "            preds.append(pred.detach().cpu())   # collect prediction  # (after append) is a list of batch tensors.\n",
        "    preds = torch.cat(preds, dim=0).numpy()     # concatenate all predictions and convert to a numpy array\n",
        "    return preds # came from 1 epoch, multiple batches, so it is a list of tensors, each tensor is a batch of predictions. After concatenation, it becomes a single numpy array of predictions for all samples in the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **1. Example change**\n",
        "\n",
        "```python\n",
        "pred = model(x)                     # forward pass (compute output)\n",
        "preds.append(pred.detach().cpu())   # collect prediction\n",
        "preds = torch.cat(preds, dim=0).numpy()\n",
        "```\n",
        "#### 1. `pred = model(x)`\n",
        "\n",
        "Suppose your batch size is 4. The model outputs a tensor (on GPU):\n",
        "\n",
        "````python\n",
        "# pred (on GPU)\n",
        "tensor([12.5,  8.3, 15.7,  9.1], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
        "````\n",
        "\n",
        "#### 2. `preds.append(pred.detach().cpu())`\n",
        "\n",
        "- `.detach()` removes gradient tracking.\n",
        "- `.cpu()` moves tensor to CPU. **NumPy arrays can only operate on CPU memory**\n",
        "\n",
        "After this, `preds` is a list of tensors (one per batch):\n",
        "\n",
        "````python\n",
        "# After first batch\n",
        "preds = [tensor([12.5,  8.3, 15.7,  9.1])]  # on CPU, no grad\n",
        "\n",
        "# After more batches, e.g. 3 batches:\n",
        "preds = [\n",
        "    tensor([12.5,  8.3, 15.7,  9.1]),\n",
        "    tensor([10.2, 11.3,  7.8, 13.4]),\n",
        "    tensor([ 9.9, 14.2,  8.7, 12.1])\n",
        "]\n",
        "````\n",
        "\n",
        "#### 3. `preds = torch.cat(preds, dim=0).numpy()`\n",
        "\n",
        "- `torch.cat(preds, dim=0)` stacks all batch tensors into one tensor: Concatenates the list of tensors along the batch dimension (dim=0), like stacking rows into one tensor\n",
        "````python\n",
        "tensor([12.5,  8.3, 15.7,  9.1, 10.2, 11.3,  7.8, 13.4,  9.9, 14.2,  8.7, 12.1])\n",
        "````\n",
        "\n",
        "- `.numpy()` converts it to a NumPy array:\n",
        "````python\n",
        "array([12.5,  8.3, 15.7,  9.1, 10.2, 11.3,  7.8, 13.4,  9.9, 14.2,  8.7, 12.1])\n",
        "````\n",
        "\n",
        "**Summary Table:**\n",
        "\n",
        "| Step                                 | Example Output (shape)                        |\n",
        "|-------------------------------------- |-----------------------------------------------|\n",
        "| `pred = model(x)`                     | tensor([12.5, 8.3, 15.7, 9.1]) (4,)           |\n",
        "| `preds.append(pred.detach().cpu())`   | [tensor([12.5, ...]), ...] (list of (4,))     |\n",
        "| `torch.cat(preds, dim=0).numpy()`     | array([12.5, 8.3, ..., 12.1]) (N,)            |\n",
        "\n",
        "**Result:**  \n",
        "You get a 1D NumPy array of all predictions for the dataset, one value per sample."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvckkF5dvf0j"
      },
      "source": [
        "# **Setup Hyper-parameters**\n",
        "\n",
        "`config` contains hyper-parameters for training and the path to save your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "id": "NPXpdumwPjE7"
      },
      "outputs": [],
      "source": [
        "device = get_device()                 # get the current available device ('cpu' or 'cuda')\n",
        "os.makedirs('models', exist_ok=True)  # The trained model will be saved to ./models/\n",
        "target_only = True                   # TODO: Using 40 states & 2 tested_positive features\n",
        "\n",
        "# TODO: How to tune these hyper-parameters to improve your model's performance?\n",
        "config = {\n",
        "    'n_epochs': 10000,                # maximum number of epochs #3000 using ealy stop so large number is fine \n",
        "    'batch_size': 200,               # mini-batch size for dataloader  #270\n",
        "    'optimizer': 'Adam',              # optimization algorithm (optimizer in torch.optim) #SGD\n",
        "    'optim_hparas': {                # hyper-parameters for the optimizer (depends on which optimizer you are using)\n",
        "        'lr': 0.0005,                 # learning rate of SGD #0.001\n",
        "        #'momentum': 0.9              # momentum for SGD #0.9\n",
        "    },\n",
        "    'early_stop': 1000,               # early stopping epochs (the number epochs since your model's last improvement)  #200\n",
        "    'save_path': 'models/model.pth'  # your model will be saved here\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **1. How to tune**  \n",
        "Here‚Äôs a practical guide for tuning and setting the hyper-parameters in your `config` dictionary, based on your dataset and typical deep learning practice:\n",
        "\n",
        "---\n",
        "\n",
        "#### 1. `n_epochs` (Maximum number of epochs)\n",
        "- **What it does:** Maximum times the model will see the whole training set.\n",
        "- **How to set:**  \n",
        "  - If you use early stopping, you can set this high (e.g., 300‚Äì3000).\n",
        "  - If your model overfits quickly, use a lower value.\n",
        "- **Tip:** Early stopping will usually stop before reaching this number.\n",
        "\n",
        "---\n",
        "\n",
        "#### 2. `batch_size`\n",
        "- **What it does:** Number of samples per gradient update.\n",
        "- **How to set:**  \n",
        "  - **Small dataset:** Use a larger batch size (e.g., 64, 128, 256, 512).\n",
        "  - **Large dataset:** Use what fits in your GPU memory (commonly 32‚Äì256).\n",
        "  - **Rule of thumb:** Try 64 or 128 first. If you have memory issues, reduce it.\n",
        "- **Effect:** Larger batch = faster but less frequent updates; smaller batch = more updates, possibly better generalization.\n",
        "\n",
        "---\n",
        "\n",
        "#### 3. `optimizer`\n",
        "- **What it does:** Optimization algorithm.\n",
        "- **How to set:**  \n",
        "  - For most DNNs, **Adam** is a good default (`'Adam'`).\n",
        "  - For simple models or if you want to experiment, try `'SGD'` with momentum.\n",
        "- **Tip:** Adam usually converges faster and is less sensitive to learning rate.\n",
        "\n",
        "---\n",
        "\n",
        "#### 4. `optim_hparas` (Learning rate, momentum, etc.)\n",
        "- **`lr` (learning rate):**\n",
        "  - **Adam:** Start with `0.001` or `0.0005`.\n",
        "  - **SGD:** Start with `0.01` or `0.1` (but you may need to tune down).\n",
        "- **`momentum`:**\n",
        "  - Only for SGD. Try `0.9` as a default.\n",
        "- **Tip:** If loss does not decrease, lower the learning rate.\n",
        "\n",
        "---\n",
        "\n",
        "#### 5. `early_stop`\n",
        "- **What it does:** Number of epochs to wait for improvement before stopping.\n",
        "- **How to set:**  \n",
        "  - If your validation loss is noisy, use a higher value (e.g., 10‚Äì20).\n",
        "  - If your model overfits quickly, use a lower value (e.g., 5).\n",
        "  - **Default:** 10 is a good starting point.\n",
        "- **Tip:** Too high = waste time; too low = might stop too early.\n",
        "\n",
        "---\n",
        "\n",
        "#### 6. `save_path`\n",
        "- **What it does:** Where to save your best model.\n",
        "- **How to set:**  \n",
        "  - Any valid path, e.g., `'models/model.pth'`.\n",
        "\n",
        "---\n",
        "\n",
        "#### Example for your dataset\n",
        "\n",
        "```python\n",
        "config = {\n",
        "    'n_epochs': 1000,                # Try 1000, early stopping will likely stop earlier\n",
        "    'batch_size': 128,               # Try 128 or 256 if your GPU can handle it\n",
        "    'optimizer': 'Adam',             # Adam is a good default\n",
        "    'optim_hparas': {\n",
        "        'lr': 0.001,                 # Adam default\n",
        "        # 'momentum': 0.9            # Only needed for SGD\n",
        "    },\n",
        "    'early_stop': 10,                # Stop if no improvement for 10 epochs\n",
        "    'save_path': 'models/model.pth'\n",
        "}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "#### **How to Tune**\n",
        "1. **Start with defaults above.**\n",
        "2. **Train and watch the learning curve.**\n",
        "3. **If loss plateaus too early:**  \n",
        "   - Lower the learning rate.\n",
        "   - Increase `early_stop`.\n",
        "4. **If overfitting:**  \n",
        "   - Lower `n_epochs` or `early_stop`.\n",
        "   - Add more regularization (e.g., increase dropout or L2 penalty).\n",
        "5. **If training is slow:**  \n",
        "   - Increase `batch_size` (if memory allows).\n",
        "6. **If validation loss is noisy:**  \n",
        "   - Increase `early_stop`.\n",
        "\n",
        "---\n",
        "\n",
        "**Summary Table:**\n",
        "\n",
        "| Hyper-parameter | Typical Value      | How to Tune                        |\n",
        "|-----------------|-------------------|-------------------------------------|\n",
        "| n_epochs        | 300‚Äì3000          | Use early stopping                  |\n",
        "| batch_size      | 64, 128, 256      | As large as fits in GPU             |\n",
        "| optimizer       | 'Adam' or 'SGD'   | Adam is usually better              |\n",
        "| lr (Adam)       | 0.001, 0.0005     | Lower if loss is unstable           |\n",
        "| lr (SGD)        | 0.01, 0.1         | Lower if loss is unstable           |\n",
        "| momentum        | 0.9               | Only for SGD                        |\n",
        "| early_stop      | 5‚Äì20              | Higher if validation loss is noisy  |\n",
        "\n",
        "---\n",
        "\n",
        "**Tip:**  \n",
        "Always monitor both training and validation loss. Adjust hyper-parameters based on their trends. Try only changing one parameter at a time for clear results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6j1eOV3TOH-j"
      },
      "source": [
        "# **Load data and model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNrYBMmePLKm",
        "outputId": "fcd4f175-4f7e-4306-f33c-5f8285f11dce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished reading the train set of COVID19 Dataset (2430 samples found, each dim = 17)\n",
            "Finished reading the dev set of COVID19 Dataset (270 samples found, each dim = 17)\n",
            "Finished reading the test set of COVID19 Dataset (893 samples found, each dim = 17)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_25908\\4152759328.py:52: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\ReduceOps.cpp:1823.)\n",
            "  / self.data[:, 40:].std(dim=0, keepdim=True)  # Z-score standardization (Standardizes each feature to mean 0, std 1)  # keepdim=True keeps the dimensions of the tensor, so the mean and std are broadcasted correctly across the features. make sure the dimensions match when performing operations like subtraction and division.\n"
          ]
        }
      ],
      "source": [
        "tr_set = prep_dataloader(tr_path, 'train', config['batch_size'], target_only=target_only)\n",
        "dv_set = prep_dataloader(tr_path, 'dev', config['batch_size'], target_only=target_only)\n",
        "tt_set = prep_dataloader(tt_path, 'test', config['batch_size'], target_only=target_only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "FHylSirLP9oh"
      },
      "outputs": [],
      "source": [
        "model = NeuralNet(tr_set.dataset.dim).to(device)  # Construct model and move to device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **1. .dataset.dim**\n",
        "Since tr_set has a Dataloader and a COVID19Dataset(Dataset) class  it calls the attribute in Dataloader to get the raw data\n",
        "In PyTorch‚Äôs source code for `DataLoader`:\n",
        "\n",
        "```python\n",
        "class DataLoader:\n",
        "    def __init__(self, dataset, ...):\n",
        "        self.dataset = dataset\n",
        "```\n",
        "\n",
        "That‚Äôs it ‚Äî `.dataset` is just an attribute set during initialization. It points back to your original `COVID19Dataset` object.\n",
        "\n",
        "\n",
        "calls the attribute of COVID19Dataset(Dataset) class to get the dimension:\n",
        "From your original code:\n",
        "\n",
        "```python\n",
        "self.dim = self.data.shape[1]\n",
        "```\n",
        "\n",
        "This line sets the `.dim` attribute to the **number of features (columns)** in your dataset ‚Äî i.e., the second dimension of the data matrix.\n",
        "\n",
        "üß† Recap: What kind of attribute is `.dim`?\n",
        "\n",
        "| Attribute | Kind                      | Where it's from                    | Description                         |\n",
        "| --------- | ------------------------- | ---------------------------------- | ----------------------------------- |\n",
        "| `dim`     | Custom instance attribute | Defined in your class‚Äôs `__init__` | Stores the number of input features |\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "### **2. When to move to device (GPU) and move back to CPU**\n",
        "Excellent question ‚Äî managing **`device` movement** (i.e., moving tensors and models between CPU and GPU) is **crucial** for efficient and correct training in PyTorch. Let's break it down clearly:\n",
        "\n",
        "\n",
        "#### ‚úÖ General Rule of Device Usage in PyTorch\n",
        "\n",
        "##### 1. **Model to device first**\n",
        "\n",
        "```python\n",
        "model = MyModel()\n",
        "model.to(device)  # Move model parameters to GPU or CPU\n",
        "```\n",
        "\n",
        "* Do this **before training begins**, usually right after model creation.\n",
        "\n",
        "\n",
        "##### 2. **Data to device inside the training loop**\n",
        "\n",
        "```python\n",
        "for x_batch, y_batch in train_loader:\n",
        "    x_batch = x_batch.to(device)\n",
        "    y_batch = y_batch.to(device)\n",
        "    ...\n",
        "```\n",
        "\n",
        "* You move **each mini-batch** to the device **right before the forward pass**.\n",
        "* Don't move the entire dataset to GPU at once ‚Äî it consumes too much memory.\n",
        "\n",
        "\n",
        "##### 3. **Tensors from model output will also be on the same device**\n",
        "\n",
        "```python\n",
        "output = model(x_batch)  # output is on the same device as model and input\n",
        "```\n",
        "\n",
        "\n",
        "##### 4. **Move data back to CPU only when needed**\n",
        "\n",
        "You typically move tensors **back to CPU** when:\n",
        "\n",
        "* Saving results to disk\n",
        "* Converting tensors to NumPy arrays for analysis or plotting\n",
        "* Logging or debugging outside GPU\n",
        "\n",
        "```python\n",
        "output_cpu = output.detach().cpu().numpy()\n",
        "```\n",
        "\n",
        "> ‚úÖ `detach()` removes the computation graph\n",
        "> ‚úÖ `cpu()` moves from GPU to CPU\n",
        "> ‚úÖ `numpy()` converts to NumPy\n",
        "\n",
        "\n",
        "##### üîÅ **Full Training Loop Example with Device Handling**\n",
        "\n",
        "```python\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = MyModel()\n",
        "model.to(device)  # Step 1: move model to device\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for x_batch, y_batch in train_loader:\n",
        "        # Step 2: move data to device\n",
        "        x_batch = x_batch.to(device)\n",
        "        y_batch = y_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(x_batch)\n",
        "        loss = loss_fn(output, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Step 3: move output back to CPU if needed\n",
        "        preds = output.detach().cpu().numpy()  # for metrics/logging\n",
        "```\n",
        "\n",
        "\n",
        "#### üß† Summary Table\n",
        "\n",
        "| Step | What to move                    | When                       | Why                                |\n",
        "| ---- | ------------------------------- | -------------------------- | ---------------------------------- |\n",
        "| 1    | `model.to(device)`              | Once before training       | So computations happen on GPU      |\n",
        "| 2    | `x.to(device)` / `y.to(device)` | Every batch                | Ensure data matches model's device |\n",
        "| 3    | `output.detach().cpu()`         | When saving/logging/output | To free GPU and use NumPy          |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sX2B_zgSOPTJ"
      },
      "source": [
        "# **Start Training!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrEbUxazQAAZ",
        "outputId": "f4f3bd74-2d97-4275-b69f-6609976b91f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving model (epoch =    1, loss = 314.1988)\n",
            "Saving model (epoch =    2, loss = 305.5474)\n",
            "Saving model (epoch =    3, loss = 303.0509)\n",
            "Saving model (epoch =    4, loss = 299.9303)\n",
            "Saving model (epoch =    8, loss = 298.4562)\n",
            "Saving model (epoch =   10, loss = 296.1494)\n",
            "Saving model (epoch =   11, loss = 292.2163)\n",
            "Saving model (epoch =   12, loss = 290.4170)\n",
            "Saving model (epoch =   13, loss = 288.5070)\n",
            "Saving model (epoch =   14, loss = 282.3583)\n",
            "Saving model (epoch =   15, loss = 279.4553)\n",
            "Saving model (epoch =   16, loss = 276.8430)\n",
            "Saving model (epoch =   17, loss = 275.1930)\n",
            "Saving model (epoch =   18, loss = 269.1803)\n",
            "Saving model (epoch =   19, loss = 268.0863)\n",
            "Saving model (epoch =   20, loss = 265.3432)\n",
            "Saving model (epoch =   21, loss = 257.0929)\n",
            "Saving model (epoch =   23, loss = 256.1613)\n",
            "Saving model (epoch =   24, loss = 250.2685)\n",
            "Saving model (epoch =   25, loss = 243.0853)\n",
            "Saving model (epoch =   28, loss = 237.3011)\n",
            "Saving model (epoch =   29, loss = 234.8892)\n",
            "Saving model (epoch =   30, loss = 230.8806)\n",
            "Saving model (epoch =   31, loss = 225.9272)\n",
            "Saving model (epoch =   34, loss = 220.5470)\n",
            "Saving model (epoch =   35, loss = 208.2801)\n",
            "Saving model (epoch =   36, loss = 203.1199)\n",
            "Saving model (epoch =   38, loss = 202.3150)\n",
            "Saving model (epoch =   39, loss = 195.8865)\n",
            "Saving model (epoch =   40, loss = 172.7281)\n",
            "Saving model (epoch =   46, loss = 165.8398)\n",
            "Saving model (epoch =   47, loss = 162.1628)\n",
            "Saving model (epoch =   48, loss = 153.0379)\n",
            "Saving model (epoch =   50, loss = 148.9102)\n",
            "Saving model (epoch =   53, loss = 144.0866)\n",
            "Saving model (epoch =   54, loss = 142.6844)\n",
            "Saving model (epoch =   55, loss = 129.3453)\n",
            "Saving model (epoch =   60, loss = 120.3498)\n",
            "Saving model (epoch =   61, loss = 109.0415)\n",
            "Saving model (epoch =   63, loss = 108.7930)\n",
            "Saving model (epoch =   65, loss = 100.9677)\n",
            "Saving model (epoch =   66, loss = 100.9241)\n",
            "Saving model (epoch =   67, loss = 92.6638)\n",
            "Saving model (epoch =   70, loss = 89.7870)\n",
            "Saving model (epoch =   73, loss = 82.6402)\n",
            "Saving model (epoch =   74, loss = 82.3967)\n",
            "Saving model (epoch =   76, loss = 80.9125)\n",
            "Saving model (epoch =   77, loss = 79.3086)\n",
            "Saving model (epoch =   78, loss = 68.2356)\n",
            "Saving model (epoch =   82, loss = 54.2896)\n",
            "Saving model (epoch =   86, loss = 45.1801)\n",
            "Saving model (epoch =   91, loss = 40.3817)\n",
            "Saving model (epoch =   92, loss = 31.2928)\n",
            "Saving model (epoch =   93, loss = 29.1928)\n",
            "Saving model (epoch =   95, loss = 26.3944)\n",
            "Saving model (epoch =  100, loss = 23.4014)\n",
            "Saving model (epoch =  103, loss = 20.2730)\n",
            "Saving model (epoch =  104, loss = 16.5825)\n",
            "Saving model (epoch =  108, loss = 15.5562)\n",
            "Saving model (epoch =  113, loss = 11.3574)\n",
            "Saving model (epoch =  117, loss = 11.2266)\n",
            "Saving model (epoch =  118, loss = 11.1150)\n",
            "Saving model (epoch =  119, loss = 11.0837)\n",
            "Saving model (epoch =  121, loss = 7.2067)\n",
            "Saving model (epoch =  126, loss = 6.3080)\n",
            "Saving model (epoch =  133, loss = 6.1104)\n",
            "Saving model (epoch =  141, loss = 5.9157)\n",
            "Saving model (epoch =  165, loss = 5.6545)\n",
            "Saving model (epoch =  178, loss = 5.4137)\n",
            "Saving model (epoch =  179, loss = 5.3053)\n",
            "Saving model (epoch =  181, loss = 5.1370)\n",
            "Saving model (epoch =  198, loss = 4.9113)\n",
            "Saving model (epoch =  200, loss = 4.8824)\n",
            "Saving model (epoch =  209, loss = 4.7929)\n",
            "Saving model (epoch =  217, loss = 4.4338)\n",
            "Saving model (epoch =  232, loss = 4.3299)\n",
            "Saving model (epoch =  237, loss = 4.2467)\n",
            "Saving model (epoch =  239, loss = 4.1878)\n",
            "Saving model (epoch =  244, loss = 4.0381)\n",
            "Saving model (epoch =  250, loss = 3.7127)\n",
            "Saving model (epoch =  261, loss = 3.4790)\n",
            "Saving model (epoch =  270, loss = 3.3012)\n",
            "Saving model (epoch =  276, loss = 3.2613)\n",
            "Saving model (epoch =  284, loss = 3.0853)\n",
            "Saving model (epoch =  285, loss = 3.0130)\n",
            "Saving model (epoch =  290, loss = 2.9895)\n",
            "Saving model (epoch =  297, loss = 2.7428)\n",
            "Saving model (epoch =  304, loss = 2.6841)\n",
            "Saving model (epoch =  311, loss = 2.6806)\n",
            "Saving model (epoch =  314, loss = 2.3699)\n",
            "Saving model (epoch =  329, loss = 2.3239)\n",
            "Saving model (epoch =  330, loss = 2.2134)\n",
            "Saving model (epoch =  343, loss = 2.1189)\n",
            "Saving model (epoch =  350, loss = 1.9754)\n",
            "Saving model (epoch =  355, loss = 1.9282)\n",
            "Saving model (epoch =  363, loss = 1.8680)\n",
            "Saving model (epoch =  376, loss = 1.8313)\n",
            "Saving model (epoch =  380, loss = 1.6587)\n",
            "Saving model (epoch =  393, loss = 1.6290)\n",
            "Saving model (epoch =  395, loss = 1.5988)\n",
            "Saving model (epoch =  404, loss = 1.5823)\n",
            "Saving model (epoch =  405, loss = 1.5501)\n",
            "Saving model (epoch =  412, loss = 1.5432)\n",
            "Saving model (epoch =  416, loss = 1.4726)\n",
            "Saving model (epoch =  419, loss = 1.4582)\n",
            "Saving model (epoch =  427, loss = 1.4095)\n",
            "Saving model (epoch =  432, loss = 1.3430)\n",
            "Saving model (epoch =  441, loss = 1.3062)\n",
            "Saving model (epoch =  444, loss = 1.2133)\n",
            "Saving model (epoch =  447, loss = 1.1936)\n",
            "Saving model (epoch =  449, loss = 1.1775)\n",
            "Saving model (epoch =  474, loss = 1.1690)\n",
            "Saving model (epoch =  478, loss = 1.1411)\n",
            "Saving model (epoch =  479, loss = 1.1390)\n",
            "Saving model (epoch =  481, loss = 1.1162)\n",
            "Saving model (epoch =  485, loss = 1.0835)\n",
            "Saving model (epoch =  487, loss = 1.0594)\n",
            "Saving model (epoch =  496, loss = 1.0422)\n",
            "Saving model (epoch =  527, loss = 1.0090)\n",
            "Saving model (epoch =  529, loss = 1.0004)\n",
            "Saving model (epoch =  538, loss = 0.9826)\n",
            "Saving model (epoch =  541, loss = 0.9638)\n",
            "Saving model (epoch =  564, loss = 0.9601)\n",
            "Saving model (epoch =  572, loss = 0.9594)\n",
            "Saving model (epoch =  576, loss = 0.9565)\n",
            "Saving model (epoch =  585, loss = 0.9513)\n",
            "Saving model (epoch =  586, loss = 0.9433)\n",
            "Saving model (epoch =  597, loss = 0.9427)\n",
            "Saving model (epoch =  598, loss = 0.9353)\n",
            "Saving model (epoch =  599, loss = 0.9319)\n",
            "Saving model (epoch =  614, loss = 0.8959)\n",
            "Saving model (epoch =  680, loss = 0.8584)\n",
            "Saving model (epoch =  704, loss = 0.8525)\n",
            "Saving model (epoch =  736, loss = 0.8485)\n",
            "Saving model (epoch =  762, loss = 0.8408)\n",
            "Saving model (epoch =  922, loss = 0.8328)\n",
            "Saving model (epoch = 1602, loss = 0.8323)\n",
            "Finished training after 2603 epochs\n"
          ]
        }
      ],
      "source": [
        "model_loss, model_loss_record = train(tr_set, dv_set, model, config, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "hsNO9nnXQBvP",
        "outputId": "1626def6-94c7-4a87-9447-d939f827c8eb"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plot_learning_curve(model_loss_record, title='deep model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "3iZTVn5WQFpX",
        "outputId": "a2d5e118-559d-45c6-b644-6792af54663d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_25908\\1425700501.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  ckpt = torch.load(config['save_path'], map_location='cpu')  # Load your best model\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 500x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "del model\n",
        "model = NeuralNet(tr_set.dataset.dim).to(device)\n",
        "ckpt = torch.load(config['save_path'], map_location='cpu')  # Load your best model\n",
        "model.load_state_dict(ckpt)\n",
        "plot_pred(dv_set, model, device)  # Show prediction on the validation set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **1. del model**\n",
        "#### ‚úÖ Why delete it?\n",
        "\n",
        "* To **clear GPU/CPU memory**, especially if training is done.\n",
        "* To ensure a **clean reload** from the saved checkpoint ‚Äî so you're only using the best weights saved to disk.\n",
        "\n",
        "---\n",
        "\n",
        "#### ‚úÖ Then you create a **new model object**:\n",
        "\n",
        "```python\n",
        "model = NeuralNet(...).to(device)\n",
        "```\n",
        "\n",
        "This new object is:\n",
        "\n",
        "* Structurally identical\n",
        "* Clean (random weights at this point)\n",
        "* Ready to be **filled with trained weights** using:\n",
        "\n",
        "```python\n",
        "model.load_state_dict(torch.load(...))\n",
        "```\n",
        "\n",
        "---\n",
        "### **2. map_location = 'cpu'**\n",
        "Excellent! Let's clearly **separate the purposes** of:\n",
        "\n",
        "1. ‚úÖ Using `map_location='cpu'` when loading a model\n",
        "2. ‚úÖ Using `.to('cpu')` to move a model or tensor to the CPU\n",
        "\n",
        "---\n",
        "\n",
        "üß© 1. **Purpose of `map_location='cpu'` in `torch.load()`**\n",
        "\n",
        "#### üéØ Main Purpose:\n",
        "\n",
        "To **safely load a checkpoint** that might have been saved on GPU, even if you‚Äôre currently **not using GPU**.\n",
        "\n",
        "#### ‚úÖ Use case:\n",
        "\n",
        "* The saved file contains CUDA tensors (e.g., saved with `.to('cuda')`)\n",
        "* You‚Äôre loading it **on a CPU-only machine**\n",
        "* You want to **prevent deserialization errors**\n",
        "\n",
        "#### üîí Example:\n",
        "\n",
        "```python\n",
        "ckpt = torch.load('model.pth', map_location='cpu')  # safely load CUDA checkpoint to CPU\n",
        "```\n",
        "\n",
        "#### ‚úÖ Summary:\n",
        "\n",
        "| Goal                          | `map_location='cpu'`        |\n",
        "| ----------------------------- | --------------------------- |\n",
        "| Load checkpoint to CPU        | ‚úÖ Yes                       |\n",
        "| Prevent device mismatch error | ‚úÖ Yes                       |\n",
        "| Needed when loading on CPU    | ‚úÖ Yes                       |\n",
        "| Moves model or tensor?        | ‚ùå No (only affects loading) |\n",
        "\n",
        "---\n",
        "\n",
        "üß© 2. **Purpose of `.to('cpu')` (or `.to(device)`)**\n",
        "\n",
        "#### üéØ Main Purpose:\n",
        "\n",
        "To **move an already existing model or tensor** to a specific device (CPU or GPU) ‚Äî after it‚Äôs been created or loaded.\n",
        "\n",
        "#### ‚úÖ Use case:\n",
        "\n",
        "* You finished loading model weights\n",
        "* Now you want to move the model to CPU (or GPU) for use\n",
        "* You‚Äôre doing **inference**, **evaluation**, or **training**\n",
        "\n",
        "### üîÑ Example:\n",
        "\n",
        "```python\n",
        "model = model.to('cpu')  # move model to CPU\n",
        "```\n",
        "\n",
        "Or:\n",
        "\n",
        "```python\n",
        "x = x.to(device)  # move tensor to CPU or GPU\n",
        "```\n",
        "\n",
        "#### ‚úÖ Summary:\n",
        "\n",
        "| Goal                         | `.to('cpu')` or `.to(device)` |\n",
        "| ---------------------------- | ----------------------------- |\n",
        "| Move model/tensor to CPU     | ‚úÖ Yes                         |\n",
        "| Needed for inference or eval | ‚úÖ Often                       |\n",
        "| Changes storage location?    | ‚úÖ Yes                         |\n",
        "| Needed before forward pass   | ‚úÖ Yes                         |\n",
        "\n",
        "---\n",
        "\n",
        "#### ‚úÖ Side-by-Side Comparison\n",
        "\n",
        "| Feature                        | `map_location='cpu'`                     | `.to('cpu')`                         |\n",
        "| ------------------------------ | ---------------------------------------- | ------------------------------------ |\n",
        "| Part of                        | `torch.load()`                           | `.to()` method                       |\n",
        "| Applies to                     | **File loading**                         | **Model or tensor objects**          |\n",
        "| Purpose                        | Load tensors to CPU regardless of origin | Move object to CPU (or other device) |\n",
        "| Prevents CUDA load errors      | ‚úÖ Yes                                    | ‚ùå No (not related to loading)        |\n",
        "| Required for CPU-only machines | ‚úÖ Yes                                    | ‚úÖ Yes (if using CPU)                 |\n",
        "| Moves live object              | ‚ùå No                                     | ‚úÖ Yes                                |\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQikz3IPiyPf"
      },
      "source": [
        "# **Testing**\n",
        "The predictions of your model on testing set will be stored at `pred.csv`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8cTuQjQQOon",
        "outputId": "6bc5de07-4c5a-4e87-9ae3-d09f539c5f2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving results to pred.csv\n"
          ]
        }
      ],
      "source": [
        "def save_pred(preds, file):\n",
        "    ''' Save predictions to specified file '''\n",
        "    print('Saving results to {}'.format(file))\n",
        "    with open(file, 'w') as fp:\n",
        "        writer = csv.writer(fp)\n",
        "        writer.writerow(['id', 'tested_positive'])\n",
        "        for i, p in enumerate(preds):\n",
        "            writer.writerow([i, p])\n",
        "\n",
        "preds = test(tt_set, model, device)  # predict COVID-19 cases with your model\n",
        "save_pred(preds, 'pred.csv')         # save prediction file to pred.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **1. alternative: pandas to save file**\n",
        "\n",
        "Rewritten with `pandas`:\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "def save_pred(preds, file):\n",
        "    ''' Save predictions to specified file using pandas '''\n",
        "    print('Saving results to {}'.format(file))\n",
        "    df = pd.DataFrame({\n",
        "        'id': range(len(preds)),\n",
        "        'tested_positive': preds\n",
        "    })\n",
        "    df.to_csv(file, index=False)\n",
        "\n",
        "\n",
        "preds = test(tt_set, model, device)  # predict COVID-19 cases with your model\n",
        "save_pred(preds, 'pred.csv')\n",
        "```\n",
        "\n",
        "\n",
        "### Output CSV:\n",
        "\n",
        "```\n",
        "id,tested_positive\n",
        "0,4.318\n",
        "1,3.215\n",
        "2,2.792\n",
        "...\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## retrospective analysis\n",
        "## **1.** :\n",
        "originally I was using  \n",
        "### **L2 regulization (not the problem)** \n",
        "#### ‚úÖ**What L2 does:**\n",
        "\n",
        "L2 adds a penalty to the loss function, proportional to the square of the weights:\n",
        "\n",
        "$$\n",
        "\\text{Loss}_{\\text{total}} = \\text{Loss}_{\\text{MSE}} + \\lambda \\sum w^2\n",
        "$$\n",
        "\n",
        "So even if your model fits the data well (low MSE), the **L2 penalty** increases the total training loss.\n",
        "\n",
        "---\n",
        "\n",
        "#### ‚úÖ Why It's Beneficial\n",
        "\n",
        "* **Discourages large weights**, which makes the model **simpler and more generalizable**.\n",
        "* Helps **prevent overfitting**, just like Dropout or data augmentation.\n",
        "* Adds a kind of **‚Äúsoft constraint‚Äù** on the model capacity, which acts like **a form of noise** during training because the model has to trade off between fitting and staying small.\n",
        "\n",
        "#### üîé Validation Loss Often Looks Lower\n",
        "\n",
        "During **validation**, the L2 term is **not included**, so:\n",
        "\n",
        "$$\n",
        "\\text{Loss}_{\\text{val}} = \\text{MSE only}\n",
        "$$\n",
        "\n",
        "This makes validation loss **appear smaller than training loss** ‚Äî same behavior as with Dropout.\n",
        "\n",
        "### **Batchnorm ‚Üí introduces noise**\n",
        "‚ö†Ô∏è Noise Interpretation\n",
        "\n",
        "BatchNorm uses **batch statistics** (mean and variance) that fluctuate from batch to batch, which introduces **random noise** into the learning process. This:\n",
        "\n",
        "* Acts like regularization\n",
        "* Helps prevent overfitting (like dropout does)\n",
        "\n",
        "But during **inference**, BatchNorm uses **running averages** instead of batch stats to ensure deterministic output.\n",
        "\n",
        "### **Dropout ‚Üí hurts training performance**   \n",
        "It drops out different neurons every batch which : introduces noise   \n",
        "On one batch during training, some useful neurons get dropped ‚Üí model‚Äôs prediction is worse ‚Üí higher training loss\n",
        "\n",
        "### **Summary:**\n",
        "all these are used to prevent overfitting, but the result shows that it did prevent overfitting, validation accuracy high , but training accuracy is lower and noisy. This is because during training, we use model.train uses all od the above mentioned techniques, while validation set uses model.eval() which automatically disables dropout and batchnorm. After deleting these my training accuracy is higher and validation accuracy is also higher.\n",
        "\n",
        "## **2.** :\n",
        "large batch size cause less noise and more stable training\n",
        "\n",
        "## **3.** :\n",
        "use only the 2 features that are most important for the prediction reduces overfitting\n",
        "\n",
        "## **4.** :\n",
        "using only the best  15 features reduce loss\n",
        "\n",
        "## **5.** :\n",
        "Adam optimizer with lr=0.0005 and is better than SGD with lr=0.001 and momentum=0.9\n",
        "\n",
        "## **6.** :\n",
        "reducing nearons from 64 to 16 improves \n",
        "\n",
        "## **7.** :\n",
        "Adding both batchnorm and dropout to the model improves the accuracy, the **trade off** is that the training loss fluctuates more and become higher \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### pass strong baseline (0.89266)\n",
        "\n",
        "<img src=\".\\img1.jpg\" width=\"600\" style=\"margin-left: ;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfrVxqJanGpE"
      },
      "source": [
        "# **Hints**\n",
        "\n",
        "## **Simple Baseline**\n",
        "* Run sample code\n",
        "\n",
        "## **Medium Baseline**\n",
        "* Feature selection: 40 states + 2 `tested_positive` (`TODO` in dataset)\n",
        "\n",
        "## **Strong Baseline**\n",
        "* Feature selection (what other features are useful?)\n",
        "* DNN architecture (layers? dimension? activation function?)\n",
        "* Training (mini-batch? optimizer? learning rate?)\n",
        "* L2 regularization\n",
        "* There are some mistakes in the sample code, can you find them?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tmCwXgpot3t"
      },
      "source": [
        "# **Reference**\n",
        "This code is completely written by Heng-Jui Chang @ NTUEE.  \n",
        "Copying or reusing this code is required to specify the original author. \n",
        "\n",
        "E.g.  \n",
        "Source: Heng-Jui Chang @ NTUEE (https://github.com/ga642381/ML2021-Spring/blob/main/HW01/HW01.ipynb)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "ML2021Spring - HW1.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "DLCV",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
